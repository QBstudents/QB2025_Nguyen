---
title: '3\. Worksheet: Basic R'
author: "Trang Nguyen; Z620: Quantitative Biodiversity, Indiana University"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
geometry: margin=2.54cm
---

## OVERVIEW

This worksheet introduces some of the basic features of the R computing environment (http://www.r-project.org).
It is designed to be used along side the **3. RStudio** handout in your binder. 
You will not be able to complete the exercises without the corresponding handout.

## Directions:
1. In the Markdown version of this document in your cloned repo, change "Student Name" on line 3 (above) with your name.
2. Complete as much of the worksheet as possible during class.
3. Use the handout as a guide; it contains a more complete description of data sets along with examples of proper scripting needed to carry out the exercises.
4. Answer questions in the  worksheet.
Space for your answers is provided in this document and is indicated by the ">" character.
If you need a second paragraph be sure to start the first line with ">".
You should notice that the answer is highlighted in green by RStudio (color may vary if you changed the editor theme). 
5. Before you leave the classroom today, you must **push** this file to your GitHub repo, at whatever stage you are. This will enable you to pull your work onto your own computer.
6. When you have completed the worksheet, **Knit** the text and code into a single PDF file by pressing the `Knit` button in the RStudio scripting panel.
This will save the PDF output in your '3.RStudio' folder.
7. After Knitting, please submit the worksheet by making a **push** to your GitHub repo and then create a **pull request** via GitHub.
Your pull request should include this file (**3.RStudio_Worksheet.Rmd**) with all code blocks filled out and questions answered) and the PDF output of `Knitr`   
(**3.RStudio_Worksheet.pdf**).

The completed exercise is due on **Wednesday, January 22^nd^, 2025 before 12:00 PM (noon)**.

## 1) HOW WE WILL BE USING R AND OTHER TOOLS

You are working in an RMarkdown (.Rmd) file.
This allows you to integrate text and R code into a single document.
There are two major features to this document: 1) Markdown formatted text and 2) "chunks" of R code.
Anything in an R code chunk will be interpreted by R when you *Knit* the document.

When you are done, you will *knit* your document together.
However, if there are errors in the R code contained in your Markdown document, you will not be able to knit a PDF file. 
If this happens, you will need to review your code, locate the source of the error(s), and make the appropriate changes.
Even if you are able to knit without issue, you should review the knitted document for correctness and completeness before you submit the Worksheet. Next to the `Knit` button in the RStudio scripting panel there is a spell checker button (`ABC`) button.

## 2) SETTING YOUR WORKING DIRECTORY

In the R code chunk below, please provide the code to: 
1) clear your R environment,
2) print your current working directory, and
3) set your working directory to your '3.RStudio' folder. 

```{r}
rm(list = ls()) # clear working directory

print(getwd()) # print current working directory
setwd(getwd())
# setwd("./3.RStudio") # set current workind to #3.RStudio

```

## 3) USING R AS A CALCULATOR

To follow up on the pre-class exercises, please calculate the following in the R code chunk below. 
Feel free to reference the **1. Introduction to version control and computing tools** handout. 

1) the volume of a cube with length, l, = 5 (volume = l^3 )
2) the area of a circle with radius, r, = 2 (area = pi * r^2). 
3) the length of the opposite side of a right-triangle given that the angle, theta, = pi/4. (radians, a.k.a. 45°) and with hypotenuse length sqrt(2) (remember: sin(theta) = opposite/hypotenuse).
4) the log (base e) of your favorite number.

```{r}
# Q1
l = 5
paste0("Volume of the cube:", l ^ 3)

# Q2 
r = 2
paste0("Area of the circle:", pi * r ^ 2)

# Q3 
h_length = sqrt(2)
theta = pi / 4
paste0("Length of the opposite side of right triangle:", theta * h_length)

# Q4
paste0("Natural log of 1 is", log(1))

```

## 4) WORKING WITH VECTORS

To follow up on the pre-class exercises, please perform the requested operations in the R-code chunks below.

### Basic Features Of Vectors

In the R-code chunk below, do the following: 
1) Create a vector `x` consisting of any five numbers.
2) Create a new vector `w` by multiplying `x` by 14 (i.e., "scalar").
3) Add `x` and `w` and divide by 15.

```{r}
# 1) Create a vector `x` consisting of any five numbers.
x = c(1,2,3,4,5)
# 2) Create a new vector `w` by multiplying `x` by 14 (i.e., "scalar").
w = 14 * x
w
# 3) Add `x` and `w` and divide by 15.
y = (x + w) / 15 
y

```

Now, do the following: 
1) Create another vector (`k`) that is the same length as `w`.
2) Multiply `k` by `x`.
3) Use the combine function to create one more vector, `d` that consists of any three elements from `w` and any four elements of `k`.

```{r}
# 1) Create another vector (`k`) that is the same length as `w`.
k = c(2,2,2,2,2)
# 2) Multiply `k` by `x`.
k * x
# 3) Use the combine function to create one more vector, `d` that consists of any three elements from `w` and any four elements of `k`.
d = c(sample(w, size=3), sample(k, size=4))
d
```

### Summary Statistics of Vectors

In the R-code chunk below, calculate the **summary statistics** (i.e., maximum, minimum, sum, mean, median, variance, standard deviation, and standard error of the mean) for the vector (`v`) provided.

```{r}
v <- c(16.4, 16.0, 10.1, 16.8, 20.5, NA, 20.2, 13.1, 24.8, 20.2, 25.0, 20.5, 30.5, 31.4, 27.1)
paste0("maximum:", max(v, na.rm = TRUE))
paste0("minimum:", min(v, na.rm = TRUE))
paste0("sum:", sum(v, na.rm = TRUE))
paste0("mean:", mean(v, na.rm = TRUE))
paste0("median:", median(v, na.rm = TRUE))
paste0("variance:", var(v, na.rm = TRUE))
paste0("std:", sd(v, na.rm = TRUE))

```

## 5) WORKING WITH MATRICES

In the R-code chunk below, do the following:
Using a mixture of Approach 1 and 2 from the **3. RStudio** handout, create a matrix with two columns and five rows.
Both columns should consist of random numbers.
Make the mean of the first column equal to 8 with a standard deviation of 2 and the mean of the second column equal to 25 with a standard deviation of 10.

```{r}

# Approach 1
c1 = rnorm(n=5, mean=8, sd=2)
c2 = rnorm(n=5, mean=25, sd=10)
m1 = cbind(c1, c2)
m1
# Approach 2
m2 = matrix(c(c1, c2), nrow = 5, ncol = 2)
m2
```

***Question 1***: What does the `rnorm` function do? 
What do the arguments in this function specify? 
Remember to use `help()` or type `?rnorm`.

> Answer 1:
> This function generates a vector or a matrix of random number drawn from the normal distribution with specified mean and standard deviation.


In the R code chunk below, do the following: 
1) Load `matrix.txt` from the **3.RStudio** data folder as matrix `m`.
2) Transpose this matrix.
3) Determine the dimensions of the transposed matrix.

```{r}
# load data
m = read.table(paste0(getwd(), "/data/matrix.txt"))

# transpose
t(m)

# check dimension
dim(t(m))

```


***Question 2***: What are the dimensions of the matrix you just transposed?

> Answer 2:
The matrix has 10 rows and 5 columns originally (before transposition).

###Indexing a Matrix

In the R code chunk below, do the following:
1) Index matrix `m` by selecting all but the third column.
2) Remove the last row of matrix `m`.

```{r}
# rename everything but the third col
colnames(m)[c(1, 2, 4, 5)] = c("N1", "N2", "N4", "N5")
m
# remove last row
m_new = m[-c(dim(m)),]
m_new

```

## 6) BASIC DATA VISUALIZATION AND STATISTICAL ANALYSIS
### Load Zooplankton Data Set

In the R code chunk below, do the following:
1) Load the zooplankton data set from the **3.RStudio** data folder.
2) Display the structure of this data set.

```{r}
meso = read.table(paste0(getwd(), "/data/zoop_nuts.txt"), sep="\t", header = TRUE)
df_zoop = as.data.frame(meso)
summary(df_zoop)
str(meso)
```

### Correlation

In the R-code chunk below, do the following:
1) Create a matrix with the numerical data in the `meso` dataframe.
2) Visualize the pairwise **bi-plots** of the six numerical variables.
3) Conduct a simple **Pearson's correlation** analysis.

```{r}
meso.num = meso[, 3:8]
# 2. Pairwise bi plots
pairs(meso.num)

# 3. Pearson's corr
cor1 = cor(meso.num)
cor1
```


***Question 3***: Describe some of the general features based on the visualization and correlation analysis above?

> Answer 3:
> Based on the pairwise plots and the Pearson correlation matrix, we see:
> Highly Correlated Variables: Variables with high positive correlations (e.g., TN and TIN, corr=0.96) show nearly diagonal patterns in their scatter plots.
> Low Correlation: Variables with low correlation (corr ~ 0) no discernible trends in their scatter plots, appearing scattered randomly.
> Negative Correlation: Variables with negative correlations (e.g., SRP vs CHLA) exhibit a slight downward slope in their scatter plots.


In the R code chunk below, do the following:
1) Redo the correlation analysis using the `corr.test()` function in the `psych` package with the following options: method = "pearson", adjust = "BH".
2) Now, redo this correlation analysis using a non-parametric method.
3) Use the print command from the handout to see the results of each correlation analysis.

```{r}

# install.packages("psych")
require(psych)
cor2 = corr.test(meso.num, method = "pearson")
print(cor2, digits=3)

cor3 = corr.test(meso.num, method = "kendall", adjust = "BH")
print(cor3, digits=3)

# install.packages("corrplot")
require(corrplot)
corrplot(cor1, method="ellipse")
```

***Question 4***: 
Describe what you learned from `corr.test`. 
Specifically, are the results sensitive to whether you use parametric (i.e., Pearson's) or non-parametric methods?
When should one use non-parametric methods instead of parametric methods?
With the Pearson's method, is there evidence for false discovery rate due to multiple comparisons? 
Why is false discovery rate important?

> Answer 4: 
> From the reference of RDocumentation, corr.test uses the cor function to find the correlations, and then applies a t-test to the individual correlations.
> The results of the outputs are sensitive to the method used (parametric vs. non-parametric). 
> Parametric methods assume that the data are normally distributed and measure linear relationships. 
> Non-parametric methods (e.g., Spearman’s correlation) do not assume normality and are more robust for non-linear relationships or data with outliers.
> One should use non-parametric methods when the data are not normally distributed or when the relationship between variables is not linear.
> With the Pearson's method, there is evidence for false discovery rate due to multiple comparisons. 
> The corr.test function adjusts for this using methods using BH correction. 
p-values are significant before adjustment but become non-significant after FDR correction, it indicates evidence of a false discovery rate due to multiple testing.
> **False discovery rate** is important because it helps to control the proportion of false positives (Type I errors) when multiple hypothesis tests are conducted simultaneously.

### Linear Regression

In the R code chunk below, do the following:
1) Conduct a linear regression analysis to test the relationship between total nitrogen (TN) and zooplankton biomass (ZP).
2) Examine the output of the regression analysis.
3) Produce a plot of this regression analysis including the following: categorically labeled points, the predicted regression line with 95% confidence intervals, and the appropriate axis labels.

```{r}
# Linear regression
fitreg = lm(ZP ~ TN, data=meso)
summary(fitreg)

# Plot 

# scatter plot
plot(meso$TN, meso$ZP, 
  ylim=c(0, 10), 
  xlim=c(500, 5000), 
  xlab="Total Nitrogen (µg/L)",
  ylab="Zooplankton Biomass (µg/L)", las=1)

# Add point labels
text(meso$TN, meso$ZP, labels=meso$NUTS, pos=3, cex=0.8)

# create variables for x-axis
newTN = seq(min(meso$TN), max(meso$TN), length.out=10)

# predict values of the variables
regline = predict(fitreg, newdata=data.frame(TN=newTN))


# Make a line that map the values with the regressed line
lines(newTN, regline, lwd=2, col="blue")

# Add confidence intervals
conf95 = predict(fitreg, newdata=data.frame(TN=newTN), 
    interval="confidence", level=0.95, type="response")
# print(conf95)
matlines(newTN, conf95[,c("lwr", "upr")], type="l", lty=2, lwd=1, col="black")
```

***Question 5***: Interpret the results from the regression model

> Answer 5:
> There is a significant positive relationship between total nitrogen (TN) and zooplankton biomass (ZP) (p < 0.05).
> Also, the model explains 55% of the variance in zooplankton biomass (adjusted R^2 = 0.55).
> I believe that this relationship is not entirely accurate. If we split data by nutrient *NUTS* variable, there hardly any trend between TN and ZP.



```{r}

```

### Analysis of Variance (ANOVA)

Using the R code chunk below, do the following:
1) Order the nutrient treatments from low to high (see handout).
2) Produce a barplot to visualize zooplankton biomass in each nutrient treatment.
3) Include error bars (+/- 1 sem) on your plot and label the axes appropriately.
4) Use a one-way analysis of variance (ANOVA) to test the null hypothesis that zooplankton biomass is affected by the nutrient treatment.


```{r}
NUTS = factor(meso$NUTS, levels=c("L", "M", "H"))
# calculate the mean of zooplancton biomass in each nutrient treatment
zp.means = tapply(meso$ZP, NUTS, mean)

# Function to calculate the sd removing the NA values
sem = function(x) {
  sd(na.omit(x)/sqrt(length(na.omit(x))))
}

# calculate the standard error 
zp.sem = tapply(meso$ZP, NUTS, sem)

bp = barplot(zp.means, ylim=c(0, round(max(meso$ZP), digits=0)),
  pch=15, cex=1.25, las=1, cex.lab=1.4, cex.axis=1.25,
  xlab="Nutrient Supply", ylab="Zooplankton Biomass (µg/L)",
  names.arg=c("Low", "Medium", "High"))

# Add error bars
arrows(x0=bp, y0=zp.means-zp.sem, 
      x1=bp, y1=zp.means+zp.sem, angle=90, code=3, length=0.1)

# ANOVA
anova1 = aov(ZP ~ NUTS, data=meso)
summary(anova1)

# Tukey's HSD test
TukeyHSD(anova1)

# Plot the residuals of the ANOVA
par(mfrow=c(2,2), mar=c(5.1, 4.1, 4.1, 2.1))
plot(anova1)
```

## SYNTHESIS: SITE-BY-SPECIES MATRIX

In the R code chunk below, load the zoops.txt data set in your **3.RStudio** data folder.

```{r}
# load data
zoops = read.table(paste0(getwd(), "/data/zoops.txt"), sep="\t", header = TRUE)
str(zoops)
# based on the outputs of str, we see that the data is a data frame with 24 variables and 11 variables ( columns).

# Check number of unique tanks ( sites)
print(paste("Number of unique tanks:", length(unique(zoops$TANK))))


```
Create a site-by-species matrix (or dataframe) that does *not* include TANK or NUTS.
> Answer:
> I'm not sure whether site-by-species matrix means a presence/absence matrix or a matrix with biomass values, so I will create both.

```{r}
# Now create a site-by-species matrix : 
# Set 'TANK' as the row names and remove it as a column (after checking that there are exactly 24 tanks)
row.names(zoops) <- zoops$TANK
zoops.site_by_species = zoops[, !(names(zoops) %in% c("TANK", "NUTS"))]
zoops.site_by_species

# Presence / Absence matrix 
zoops.binary = zoops.site_by_species
zoops.binary[zoops.binary  > 0] = 1
zoops.binary 

```

The remaining columns of data refer to the biomass (µg/L) of different zooplankton taxa: 
  
  + CAL = calanoid copepods
  
  + DIAP = *Diaphanasoma* sp. 
  
  + CYL = cyclopoid copepods
  
  + BOSM = *Bosmina* sp.
  
  + SIMO = *Simocephallus* sp.
  
  + CERI = *Ceriodaphnia* sp.
  
  + NAUP = naupuli (immature copepod)
  
  + DLUM = *Daphnia lumholtzi*
  
  + CHYD = *Chydorus* sp. 

***Question 6***: With the visualization and statistical tools that we learned about in the **3. RStudio** handout, use the site-by-species matrix to assess whether and how different zooplankton taxa were responsible for the total biomass (ZP) response to nutrient enrichment. 
Describe what you learned below in the "Answer" section and include appropriate code in the R chunk.

> Pre-Answer:
> Here is how I understood the question and how I solve the question
> Previously, through ANOVA, we saw how different nutrient enrichments influence the total biomass, but we didnt take into account the fact that each tank has different taxa distribution.
> In this question, we want to know how each taxa contribute to the total biomass

```{r}
# First, let's calculate the total biomass for each tank
zoops$Total_Biomass = rowSums(zoops[, !(names(zoops) %in% c("TANK", "NUTS"))])

# Order category : Low, Med, High nutrient level
zoops$NUTS = factor(zoops$NUTS, levels=c("L", "M", "H"))

# Summarize the data to check differences in Total_Biomass by nutrient levels 
# We did this for zoop_nuts but instead of aggregate, we did anova
aggregate(Total_Biomass ~ NUTS, data = zoops, summary)


# Now, I create a boxplot of total biomass by nutrient level
boxplot(Total_Biomass ~ NUTS, data = zoops, 
        main = "Total Biomass (ZP) by Nutrient Levels", 
        xlab = "Nutrient Level (NUTS)", 
        ylab = "Total Biomass (µg/L)", 
        col = "lightblue")

# We get what we expected to see, higher nutrient level means higher total biomass.
# -----------------------------------------------------------------------------------

# Now I'll subset the matrix to include only biomass data of taxa
zoops_taxa <- zoops[, !(names(zoops) %in% c("NUTS", "TANKS", "Total_Biomass"))]

# Transpose the data for stacked barplot (taxa as rows) - visually, it gives us an intuition
taxa_matrix <- t(as.matrix(zoops_taxa))

# Colors for taxa
taxa_colors <- rainbow(nrow(taxa_matrix))  # Generate unique colors for each taxon

# Create the stacked bar plot
barplot(
  taxa_matrix,
  beside = FALSE,                     # Stacked bars
  col = taxa_colors,                  # Colors for each taxon
  legend.text = rownames(taxa_matrix),# Add legend for taxa
  args.legend = list(x = "topright"), # Legend position
  xlab = "Sites",                     # X-axis label
  ylab = "Biomass (µg/L)",            # Y-axis label
  main = "Contribution of Zooplankton Taxa to Total Biomass"
)

## Now, I tackle the question : what is the contribution of each species to the total biomass
# ANOVA to test if total biomass differs by nutrient level
zp_anova <- aov(Total_Biomass ~ NUTS, data = zoops) # copied from previous dataset meso
summary(zp_anova)

# The output shows that the total biomass significantly differs by nutrient level (p < 0.05).
# Linear regression to determine how each taxa contributes to total biomass
lm_model <- lm(Total_Biomass ~ CAL + DIAP + CYCL + BOSM + SIMO + CERI + NAUP + DLUM + CHYD, data = zoops)
summary(lm_model)


```

> Answer :
> For the ANOVA model of effect of nutrient enrichment on total biomass. The p-value of F test is 0.000373 < 0.05, meaning that nutrient levels (NUTS) significantly affect total biomass.
>
> For the linear regression model of different taxa on total biomass: if we look at the intercept, which is the fixed effect contribution beyond the taxa. The p-value is 0.482, so not significant. 
> Then, when we look and the coefficients of all taxa, we see that all of them have coefficients around 1, which means that their biomass contributes equally and linearly, and all the coefficients are significant.
> Finally, if we look at the explanatory power of this current model: we can see that R square is 1.0, which means that the model perfectly explains the total biomass, this was accompagnied with a low p value of F stats, which indicates that the model is significant. 


Use Knitr to create a PDF of your completed **3.RStudio_Worksheet.Rmd** document, push the repo to GitHub, and create a pull request.
Please make sure your updated repo include both the PDF and RMarkdown files.

This assignment is due on **Wednesday, January 22^nd^, 2025 at 12:00 PM (noon)**.

